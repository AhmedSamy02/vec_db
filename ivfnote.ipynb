{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Al Gazzar\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 240\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;28msum\u001b[39m(run_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(run_time)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# db = VecDB(new_db=False)\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mVecDB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     all_db \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mget_all_rows()\n\u001b[0;32m    244\u001b[0m     res \u001b[38;5;241m=\u001b[39m run_queries(db, all_db, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 91\u001b[0m, in \u001b[0;36mVecDB.__init__\u001b[1;34m(self, database_file_path, index_file_path, new_db, db_size)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path):\n\u001b[0;32m     90\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_path)\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 97\u001b[0m, in \u001b[0;36mVecDB.generate_database\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m     95\u001b[0m vectors \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mrandom((size, DIMENSION), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_vectors_to_file(vectors)\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 171\u001b[0m, in \u001b[0;36mVecDB._build_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# Placeholder for index building logic\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_rows()\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39msave_model()\n",
      "Cell \u001b[1;32mIn[39], line 37\u001b[0m, in \u001b[0;36mIVF.train\u001b[1;34m(self, vectors)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Perform KMeans clustering to find the centroids\u001b[39;00m\n\u001b[0;32m     36\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlist, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massignments \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assign each vector to a cluster\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentroids \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_  \u001b[38;5;66;03m# Get the cluster centroids\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Build the posting lists: a dictionary where key is the cluster index, and value is a list of vector indices\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1033\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \n\u001b[0;32m   1013\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1461\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1457\u001b[0m best_inertia, best_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init):\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[1;32m-> 1461\u001b[0m     centers_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_centroids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m   1465\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:989\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[1;34m(self, X, x_squared_norms, init, random_state, init_size, n_centroids)\u001b[0m\n\u001b[0;32m    986\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 989\u001b[0m     centers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_kmeans_plusplus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    996\u001b[0m     seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mpermutation(n_samples)[:n_clusters]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:234\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[1;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    231\u001b[0m np\u001b[38;5;241m.\u001b[39mclip(candidate_ids, \u001b[38;5;28;01mNone\u001b[39;00m, closest_dist_sq\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39mcandidate_ids)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Compute distances to center candidates\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[0;32m    239\u001b[0m np\u001b[38;5;241m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\pairwise.py:366\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    361\u001b[0m         YY \u001b[38;5;241m=\u001b[39m row_norms(Y, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# To minimize precision issues with float32, we compute the distance\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# matrix on chunks of X and Y upcast to float64\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43m_euclidean_distances_upcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[0;32m    369\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m safe_sparse_dot(X, Y\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\pairwise.py:562\u001b[0m, in \u001b[0;36m_euclidean_distances_upcast\u001b[1;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[0;32m    559\u001b[0m     d \u001b[38;5;241m=\u001b[39m distances[y_slice, x_slice]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 562\u001b[0m     Y_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_slice\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m YY \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    564\u001b[0m         YY_chunk \u001b[38;5;241m=\u001b[39m row_norms(Y_chunk, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Annotated\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "DB_SEED_NUMBER = 42\n",
    "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
    "DIMENSION = 70\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "\n",
    "class IVF:\n",
    "    def __init__(self, nlist: int):\n",
    "        \"\"\"\n",
    "        :param nlist: Number of Voronoi cells (partitions)\n",
    "        \"\"\"\n",
    "        self.nlist = nlist  # Number of Voronoi cells\n",
    "        self.centroids = None  # IVF centroids (cluster centers)\n",
    "        self.assignments = []  # Vector assignments to clusters\n",
    "        self.vectors = None  # Store original vectors\n",
    "        self.posting_lists = None  # Inverted file lists for each cluster\n",
    "\n",
    "    def train(self, vectors: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the IVF model to the input vectors by clustering them into 'nlist' clusters.\n",
    "        \n",
    "        :param vectors: A 2D numpy array with shape (n, d), where n is the number of vectors and d is their dimensionality.\n",
    "        \"\"\"\n",
    "        n, d = vectors.shape\n",
    "        self.vectors = vectors  # Store original vectors\n",
    "\n",
    "        # Perform KMeans clustering to find the centroids\n",
    "        kmeans = KMeans(n_clusters=self.nlist, random_state=42)\n",
    "        self.assignments = kmeans.fit_predict(vectors)  # Assign each vector to a cluster\n",
    "        self.centroids = kmeans.cluster_centers_  # Get the cluster centroids\n",
    "\n",
    "        # Build the posting lists: a dictionary where key is the cluster index, and value is a list of vector indices\n",
    "        self.posting_lists = {i: [] for i in range(self.nlist)}  \n",
    "        for i, label in enumerate(self.assignments):\n",
    "            self.posting_lists[label].append(i)\n",
    "\n",
    "    def search(self, query: np.ndarray, top_k: int) -> list:\n",
    "        \"\"\"\n",
    "        Perform a search for the 'top_k' closest vectors to the query vector.\n",
    "        \n",
    "        :param query: A 1D numpy array representing the query vector.\n",
    "        :param top_k: The number of top nearest vectors to return.\n",
    "        \n",
    "        :return: A list of the top_k closest vectors from the dataset.\n",
    "        \"\"\"\n",
    "        # Find the closest centroid to the query vector\n",
    "        distances_to_centroids = np.linalg.norm(self.centroids - query, axis=1)\n",
    "        nearest_centroid_idx = np.argmin(distances_to_centroids)\n",
    "\n",
    "        # Get the candidate vectors assigned to the closest centroid\n",
    "        candidate_indices = self.posting_lists[nearest_centroid_idx]\n",
    "\n",
    "        return candidate_indices\n",
    "\n",
    "    def save_model(self, filepath='ivfpq_model.dat'):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'centroids': self.centroids,\n",
    "                'inverted_lists': self.posting_lists\n",
    "            }, f)\n",
    "\n",
    "    def load_model(self, filepath='ivfpq_model.dat'):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.centroids = data['centroids']\n",
    "            self.posting_lists = data['inverted_lists']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VecDB:\n",
    "    def __init__(self, database_file_path = \"saved_db.dat\", index_file_path = \"index.dat\", new_db = True, db_size = None) -> None:\n",
    "        self.db_path = database_file_path\n",
    "        self.index_path = index_file_path\n",
    "        self.index = IVF(500)\n",
    "        self.index.load_model()\n",
    "        if new_db:\n",
    "            if db_size is None:\n",
    "                raise ValueError(\"You need to provide the size of the database\")\n",
    "            # delete the old DB file if exists\n",
    "            if os.path.exists(self.db_path):\n",
    "                os.remove(self.db_path)\n",
    "            self.generate_database(db_size)\n",
    "    \n",
    "    def generate_database(self, size: int) -> None:\n",
    "        rng = np.random.default_rng(DB_SEED_NUMBER)\n",
    "        vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
    "        self._write_vectors_to_file(vectors)\n",
    "        self._build_index()\n",
    "\n",
    "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
    "        mmap_vectors[:] = vectors[:]\n",
    "        mmap_vectors.flush()\n",
    "\n",
    "    def _get_num_records(self) -> int:\n",
    "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
    "\n",
    "    def insert_records(self, rows: Annotated[np.ndarray, (int, 70)]):\n",
    "        num_old_records = self._get_num_records()\n",
    "        num_new_records = len(rows)\n",
    "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
    "        mmap_vectors[num_old_records:] = rows\n",
    "        mmap_vectors.flush()\n",
    "        #TODO: might change to call insert in the index, if you need\n",
    "        self._build_index()\n",
    "\n",
    "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
    "        # This function is only load one row in memory\n",
    "        try:\n",
    "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
    "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
    "            return np.array(mmap_vector[0])\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "\n",
    "    def get_all_rows(self) -> np.ndarray:\n",
    "        # Take care this load all the data in memory\n",
    "        num_records = self._get_num_records()\n",
    "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def retrieve(self, query: Annotated[np.ndarray, (1, DIMENSION)], top_k = 5):\n",
    "        # scores = []\n",
    "        # num_records = self.index.search(query, top_k)\n",
    "        # # here we assume that the row number is the ID of each vector\n",
    "        # for i, row_num in enumerate(num_records):\n",
    "        #     vector = self.get_one_row(row_num)\n",
    "        #     score = self._cal_score(query, vector)\n",
    "        #     scores.append((score, row_num))\n",
    "        # # here we assume that if two rows have the same score, return the lowest ID\n",
    "        # scores = sorted(scores, reverse=True)[:top_k]\n",
    "        # indx=[idx for _, idx in scores ]\n",
    "        # return indx\n",
    "        indices = self.index.search(query, top_k)  # Retrieve more than top_k for better selection.\n",
    "    \n",
    "        maxvalues = [float('-inf')] * top_k\n",
    "        top_indices = [-1] * top_k\n",
    "        for i, index in enumerate(indices):\n",
    "            vector = self.get_one_row(index)\n",
    "            score = self._cal_score(query, vector)\n",
    "                    # Find the position of the smallest score in maxvalues\n",
    "            min_score_index = maxvalues.index(min(maxvalues))\n",
    "            if score > maxvalues[min_score_index]:\n",
    "                maxvalues[min_score_index] = score\n",
    "                top_indices[min_score_index] = index\n",
    "                # Combine the scores and indices, sort by score (descending), and extract indices\n",
    "        combined = sorted(zip(maxvalues, top_indices), reverse=True)\n",
    "        \n",
    "        return [idx for _, idx in combined if idx != -1]\n",
    "    \n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "\n",
    "    def _build_index(self):\n",
    "        # Placeholder for index building logic\n",
    "        vectors = self.get_all_rows()\n",
    "        self.index.train(vectors)\n",
    "        self.index.save_model()\n",
    "\n",
    "\n",
    "# This snippet of code is to show you a simple evaluate for VecDB class, but the full evaluation for project on the Notebook shared with you.\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    run_time: float\n",
    "    top_k: int\n",
    "    db_ids: List[int]\n",
    "    actual_ids: List[int]\n",
    "def monitor_retrieve_memory(db, query, top_k=5):\n",
    "    mem_usage = memory_usage(\n",
    "        (db.retrieve, (query,), {'top_k': top_k}),\n",
    "        interval=0.1,\n",
    "        retval=True\n",
    "    )\n",
    "    peak_memory_used = max(mem_usage[0]) - min(mem_usage[0])\n",
    "    result = mem_usage[1]  # The actual result from retrieve\n",
    "    print(f\"Peak memory used during retrieve: {peak_memory_used:.2f} MiB\")\n",
    "    return result\n",
    "\n",
    "def run_queries(db, np_rows, top_k, num_runs):\n",
    "    results = []\n",
    "    peak_mem_usage =0\n",
    "    for _ in range(num_runs):\n",
    "        query = np.random.random((1,70))\n",
    "        \n",
    "        tic = time.time()\n",
    "        db_ids = db.retrieve(query, top_k)\n",
    "        toc = time.time()\n",
    "        run_time = toc - tic\n",
    "        peak_mem_usage += max(memory_usage((db.retrieve, (query,), {'top_k': 5}), interval=0.1)) - min(memory_usage())\n",
    "        tic = time.time()\n",
    "        actual_ids = np.argsort(np_rows.dot(query.T).T / (np.linalg.norm(np_rows, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]\n",
    "        toc = time.time()\n",
    "        np_run_time = toc - tic\n",
    "        \n",
    "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
    "    print(f\"Memory Used: {peak_mem_usage:.2f} MiB\")\n",
    "    return results\n",
    "\n",
    "def eval(results: List[Result]):\n",
    "    # scores are negative. So getting 0 is the best score.\n",
    "    scores = []\n",
    "    run_time = []\n",
    "    for res in results:\n",
    "        run_time.append(res.run_time)\n",
    "        # case for retrieving number not equal to top_k, score will be the lowest\n",
    "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
    "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
    "            continue\n",
    "        score = 0\n",
    "        for id in res.db_ids:\n",
    "            try:\n",
    "                ind = res.actual_ids.index(id)\n",
    "                if ind > res.top_k * 3:\n",
    "                    score -= ind\n",
    "            except:\n",
    "                score -= len(res.actual_ids)\n",
    "        scores.append(score)\n",
    "\n",
    "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # db = VecDB(new_db=False)\n",
    "    db = VecDB(db_size=10**6)\n",
    "\n",
    "    all_db = db.get_all_rows()\n",
    "\n",
    "    res = run_queries(db, all_db, 5, 10)\n",
    "    print(eval(res))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "class MergedIVFPQ:\n",
    "    def __init__(self, d, nlist, m, bits_per_subvector):\n",
    "        \"\"\"\n",
    "        Initialize the IVFPQ index.\n",
    "        :param d: Dimensionality of the input vectors\n",
    "        :param nlist: Number of Voronoi cells (clusters) in IVF\n",
    "        :param m: Number of sub-vectors for Product Quantization (PQ)\n",
    "        :param bits_per_subvector: Number of bits for each subvector's quantization\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.nlist = nlist\n",
    "        self.m = m\n",
    "        self.k = 2 ** bits_per_subvector  # Number of centroids per subvector\n",
    "        self.subvector_dim = d // m\n",
    "\n",
    "        assert d % m == 0, \"Dimensionality must be divisible by the number of subvectors (m).\"\n",
    "\n",
    "        # Initialize KMeans for IVF and PQ\n",
    "        self.kmeans = MiniBatchKMeans(n_clusters=self.nlist, init='k-means++', n_init=20,batch_size=100000, max_iter=500 )\n",
    "        self.pq_codebooks = [MiniBatchKMeans(n_clusters=self.k,init='k-means++', n_init=20,batch_size=100000, max_iter=500) for _ in range(m)]\n",
    "\n",
    "        self.inverted_lists = {i: [] for i in range(nlist)}  # Inverted file structure\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"\n",
    "        Train the IVF and PQ models on the given data.\n",
    "        :param data: Input data array of shape (N, D)\n",
    "        \"\"\"\n",
    "        self.kmeans.fit(data)\n",
    "        self.centroids = self.kmeans.cluster_centers_\n",
    "        \n",
    "        # Form the codebook for each subvector\n",
    "        for i in range(self.m):\n",
    "            subvectors = data[:, i * self.subvector_dim: (i + 1) * self.subvector_dim]\n",
    "            self.pq_codebooks[i].fit(subvectors)\n",
    "\n",
    "    def encode(self, data):\n",
    "        \"\"\"\n",
    "        Encode data into PQ codes.\n",
    "        :param data: Input data array of shape (N, D)\n",
    "        :return: PQ codes for the data\n",
    "        \"\"\"\n",
    "        cluster_ids = self.kmeans.predict(data)  # Predict cluster IDs for all data\n",
    "        pq_codes = np.zeros((data.shape[0], self.m), dtype=np.int32) \n",
    "\n",
    "        for idx, cluster_id in enumerate(cluster_ids):\n",
    "            residual = data[idx] - self.centroids[cluster_id]\n",
    "            for i in range(self.m):\n",
    "                subvector = residual[i * self.subvector_dim: (i + 1) * self.subvector_dim].reshape(1, -1)\n",
    "                centroid_vectors = self.pq_codebooks[i].cluster_centers_\n",
    "\n",
    "                subvector_norm = norm(subvector)\n",
    "                centroid_norms = norm(centroid_vectors, axis=1)\n",
    "\n",
    "                if subvector_norm > 0:\n",
    "                    similarities = np.dot(centroid_vectors, subvector.T) / (centroid_norms[:, np.newaxis] * subvector_norm)\n",
    "                    similarities[np.isnan(similarities)] = -np.inf  # Handle NaN values\n",
    "                    pq_codes[idx, i] = np.argmax(similarities)\n",
    "                else:\n",
    "                    pq_codes[idx, i] = 0  # Assign a default centroid if the subvector is zero\n",
    "# pq_code stores the index for the codebook centers \n",
    "            self.inverted_lists[cluster_id].append((idx, pq_codes[idx]))\n",
    "\n",
    "        return pq_codes\n",
    "\n",
    "    def encode_single(self, vector):\n",
    "        \"\"\"\n",
    "        Encode a single query vector.\n",
    "        :param vector: Query vector of shape (D,)\n",
    "        :return: (cluster_id, pq_code) for the query vector\n",
    "        \"\"\"\n",
    "        cluster_id = self.kmeans.predict(vector.reshape(1, -1))[0]\n",
    "        residual = vector - self.centroids[cluster_id]\n",
    "        pq_code = np.zeros((self.m,), dtype=np.int32)\n",
    "\n",
    "        for i in range(self.m):\n",
    "            subvector = residual[i * self.subvector_dim: (i + 1) * self.subvector_dim].reshape(1, -1)\n",
    "            centroid_vectors = self.pq_codebooks[i].cluster_centers_\n",
    "\n",
    "            subvector_norm = norm(subvector)\n",
    "            centroid_norms = norm(centroid_vectors, axis=1)\n",
    "\n",
    "            if subvector_norm > 0:\n",
    "                similarities = np.dot(centroid_vectors, subvector.T) / (centroid_norms[:, np.newaxis] * subvector_norm)\n",
    "                similarities[np.isnan(similarities)] = -np.inf  # Handle NaN values\n",
    "                pq_code[i] = np.argmax(similarities)\n",
    "            else:\n",
    "                pq_code[i] = 0  # Assign a default centroid if the subvector is zero\n",
    "\n",
    "        return cluster_id, pq_code\n",
    "\n",
    "    def search(self, query, top_k, nprobe=1):\n",
    "        \"\"\"\n",
    "        Search for the top_k nearest neighbors to the query.\n",
    "        :param query: Query vector of shape (1, D)\n",
    "        :param top_k: Number of nearest neighbors to return\n",
    "        :param nprobe: Number of clusters to probe\n",
    "        :return: List of indices of the nearest neighbors\n",
    "        \"\"\"\n",
    "        query = query.reshape(1, -1)\n",
    "        \n",
    "        # Calculate distances to each centroid and get the closest clusters\n",
    "        cluster_distances = np.linalg.norm(self.centroids - query, axis=1)\n",
    "        nearest_clusters = np.argsort(cluster_distances)[:nprobe]\n",
    "        nearest_centroid = self.centroids[nearest_clusters[0]]\n",
    "        residual = query - nearest_centroid  # Residual vector\n",
    "        pq_similarities = []\n",
    "\n",
    "        # Iterate over the nearest clusters\n",
    "        for cluster_id in nearest_clusters:\n",
    "            candidates = self.inverted_lists[cluster_id]\n",
    "            \n",
    "            for idx, pq_code in candidates:\n",
    "                # Reconstruct the vector from the PQ code\n",
    "                reconstructed_vector = self.reconstruct(pq_code, cluster_id) # center values for the vector\n",
    "                \n",
    "                # Calculate similarity (or distance) between query and reconstructed vector\n",
    "                similarity = self.compute_similarity(residual, reconstructed_vector)\n",
    "                \n",
    "                # Append the result (index and similarity)\n",
    "                pq_similarities.append((idx, similarity))\n",
    "        \n",
    "        # Sort by similarity in descending order\n",
    "        pq_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return the indices of the top_k nearest neighbors\n",
    "        return [idx for idx, _ in pq_similarities[:top_k]]\n",
    "\n",
    "    def reconstruct(self, pq_code, cluster_id):\n",
    "        \"\"\"\n",
    "        Reconstruct the original vector from the PQ code.\n",
    "        :param pq_code: The PQ code (list of centroid indices for each subvector)\n",
    "        :param cluster_id: The cluster ID where the data point belongs (used to access centroids)\n",
    "        :return: The reconstructed vector\n",
    "        \"\"\"\n",
    "        reconstructed_vector = []\n",
    "        \n",
    "        # Reconstruct the vector by taking centroids corresponding to the PQ code\n",
    "        for i in range(self.m):\n",
    "            centroid_sub = self.pq_codebooks[i].cluster_centers_[pq_code[i]]\n",
    "            reconstructed_vector.append(centroid_sub)\n",
    "        \n",
    "        return np.hstack(reconstructed_vector)  # Concatenate subvectors to form the full vector\n",
    "\n",
    "    def compute_similarity(self, query, reconstructed_vector):\n",
    "        \"\"\"\n",
    "        Compute the similarity (e.g., cosine similarity) between the query and the reconstructed vector.\n",
    "        :param query: The query vector\n",
    "        :param reconstructed_vector: The reconstructed vector from the PQ code\n",
    "        :return: The computed similarity\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(query, reconstructed_vector)\n",
    "        norm_query = norm(query)\n",
    "        norm_reconstructed = norm(reconstructed_vector)\n",
    "        \n",
    "        if norm_query > 0 and norm_reconstructed > 0:\n",
    "            similarity = dot_product / (norm_query * norm_reconstructed)\n",
    "        else:\n",
    "            similarity = -1  # If either vector has zero norm, assign a very low similarity\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "    def save_model(self, filepath='ivfpq_model.dat'):\n",
    "        \"\"\"\n",
    "        Save the trained IVFPQ model to a .dat file.\n",
    "        :param filepath: Path to the file where the model will be saved\n",
    "        \"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'centroids': self.centroids,\n",
    "                'pq_codebooks': [codebook.cluster_centers_ for codebook in self.pq_codebooks],\n",
    "                'inverted_lists': self.inverted_lists\n",
    "            }, f)\n",
    "\n",
    "    def load_model(self, filepath='ivfpq_model.dat'):\n",
    "        \"\"\"\n",
    "        Load the IVFPQ model from a .dat file.\n",
    "        :param filepath: Path to the file where the model is saved\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.centroids = data['centroids']\n",
    "\n",
    "            # Reinitialize and set the PQ codebook centroids\n",
    "            for i, codebook_centers in enumerate(data['pq_codebooks']):\n",
    "                self.pq_codebooks[i].cluster_centers_ = codebook_centers\n",
    "\n",
    "            self.inverted_lists = data['inverted_lists']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "import numpy as np\n",
    "import os\n",
    "from b import MergedIVFPQ\n",
    "\n",
    "DB_SEED_NUMBER = 42\n",
    "ELEMENT_SIZE = np.dtype(np.float32).itemsize\n",
    "DIMENSION = 70\n",
    "\n",
    "class VecDB:\n",
    "    def __init__(self, database_file_path=\"saved_db.dat\", index_file_path=\"index.dat\", new_db=True, db_size=None) -> None:\n",
    "        self.db_path = database_file_path\n",
    "        self.index_path = index_file_path\n",
    "        # self.ivfpq = IVFPQ(nlist=150, m=4, nbits=8)\n",
    "        self.ivfpq =  MergedIVFPQ(d=DIMENSION,nlist=5000, m=35, bits_per_subvector=8)\n",
    "        # self.ivfpq =  MergedIVFPQ(d=DIMENSION,nlist=10000, m=70, bits_per_subvector=8) 10**4 0.0 0.09\n",
    "\n",
    "        self._build_index(self.get_all_rows())\n",
    "        if new_db:\n",
    "            if db_size is None:\n",
    "                raise ValueError(\"You need to provide the size of the database\")\n",
    "            # delete the old DB file if exists\n",
    "            if os.path.exists(self.db_path):\n",
    "                os.remove(self.db_path)\n",
    "            self.generate_database(db_size)\n",
    "    \n",
    "    def generate_database(self, size: int) -> None:\n",
    "        rng = np.random.default_rng(DB_SEED_NUMBER)\n",
    "        vectors = rng.random((size, DIMENSION), dtype=np.float32)\n",
    "        self._write_vectors_to_file(vectors)\n",
    "        self._build_index(vectors)\n",
    "    \n",
    "    def _write_vectors_to_file(self, vectors: np.ndarray) -> None:\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='w+', shape=vectors.shape)\n",
    "        mmap_vectors[:] = vectors[:]\n",
    "        mmap_vectors.flush()\n",
    "    \n",
    "    def _get_num_records(self) -> int:\n",
    "        return os.path.getsize(self.db_path) // (DIMENSION * ELEMENT_SIZE)\n",
    "    \n",
    "    def insert_records(self, rows: Annotated[np.ndarray, (int, 70)]):\n",
    "        num_old_records = self._get_num_records()\n",
    "        num_new_records = len(rows)\n",
    "        full_shape = (num_old_records + num_new_records, DIMENSION)\n",
    "        mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r+', shape=full_shape)\n",
    "        mmap_vectors[num_old_records:] = rows\n",
    "        mmap_vectors.flush()\n",
    "        self._build_index(mmap_vectors)  # Rebuild index after inserting new records\n",
    "    \n",
    "    def get_one_row(self, row_num: int) -> np.ndarray:\n",
    "        try:\n",
    "            offset = row_num * DIMENSION * ELEMENT_SIZE\n",
    "            mmap_vector = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(1, DIMENSION), offset=offset)\n",
    "            return np.array(mmap_vector[0])\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "    \n",
    "    def get_all_rows(self) -> np.ndarray:\n",
    "        num_records = self._get_num_records()\n",
    "        vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', shape=(num_records, DIMENSION))\n",
    "        return np.array(vectors)\n",
    "    def get_batch(self, start_idx: int, end_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Load a batch of vectors from the database using indices.\n",
    "\n",
    "        :param start_idx: Starting index (inclusive)\n",
    "        :param end_idx: Ending index (exclusive)\n",
    "        :return: A NumPy array containing the batch of vectors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            num_records = self._get_num_records()\n",
    "            # Ensure indices are within bounds\n",
    "            if start_idx < 0 or end_idx > num_records or start_idx >= end_idx:\n",
    "                raise IndexError(\"Invalid start or end index for batch retrieval.\")\n",
    "\n",
    "            # Calculate the number of rows to fetch and their byte offset\n",
    "            num_rows = end_idx - start_idx\n",
    "            offset = start_idx * DIMENSION * ELEMENT_SIZE\n",
    "\n",
    "            # Memory-map the batch into memory\n",
    "            mmap_vectors = np.memmap(self.db_path, dtype=np.float32, mode='r', \n",
    "                                     shape=(num_rows, DIMENSION), offset=offset)\n",
    "\n",
    "            # Convert to a standard NumPy array for easier handling\n",
    "            return np.array(mmap_vectors)\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {e}\"\n",
    "    def retrieve_old(self, query: Annotated[np.ndarray, (1, DIMENSION)], top_k = 5):\n",
    "        scores = []\n",
    "        num_records = self._get_num_records()\n",
    "        # here we assume that the row number is the ID of each vector\n",
    "        for row_num in range(num_records):\n",
    "            vector = self.get_one_row(row_num)\n",
    "            score = self._cal_score(query, vector)\n",
    "            scores.append((score, row_num))\n",
    "        # here we assume that if two rows have the same score, return the lowest ID\n",
    "        scores = sorted(scores, reverse=True)[:top_k]\n",
    "        return [s[1] for s in scores]\n",
    "    def retrieve(self, query: Annotated[np.ndarray, (1, DIMENSION)], top_k=5):\n",
    "        print(\"Retrieving\")\n",
    "        # candidates = self.ivfpq.search(query, top_k)\n",
    "        # return [self.get_all_rows().tolist().index(candidate.tolist()) for candidate in candidates]\n",
    "        # query /= np.linalg.norm(query, axis=1, keepdims=True)\n",
    "\n",
    "        return self.ivfpq.search(query,top_k, nprobe=100)\n",
    "    \n",
    "    def _cal_score(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return cosine_similarity\n",
    "    \n",
    "    def _build_index(self, vectors: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Incrementally builds the IVFPQ index using batches of vectors.\n",
    "        \"\"\"\n",
    "        # vectors = self.get_all_rows()\n",
    "        # vectors /= np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        print( \"Building index\")\n",
    "        self.ivfpq.train(vectors)\n",
    "        print(\"fitting\")\n",
    "        self.ivfpq.encode(vectors)\n",
    "        print(\"done\")\n",
    "\n",
    "        # num_records = self._get_num_records()\n",
    "        # step = max(num_records // 1, 1)\n",
    "        # for i in range(0, num_records, step):\n",
    "        #     batch_vectors = self.get_batch(i, min(i + step, num_records))\n",
    "        #     batch_vectors /= np.linalg.norm(batch_vectors, axis=1, keepdims=True)\n",
    "\n",
    "        #     print(\"Training IVFPQ index on batch...\")\n",
    "        #     self.ivfpq.train_batch(batch_vectors)\n",
    "        #     print(\"Encoding batch...\")\n",
    "        #     self.ivfpq.encode_batch(batch_vectors)\n",
    "        #     print(\"Finished batch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet of code is to show you a simple evaluate for VecDB class, but the full evaluation for project on the Notebook shared with you.\n",
    "import numpy as np\n",
    "from vec_db import VecDB\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    run_time: float\n",
    "    top_k: int\n",
    "    db_ids: List[int]\n",
    "    actual_ids: List[int]\n",
    "\n",
    "def run_queries(db, np_rows, top_k, num_runs):\n",
    "    results = []\n",
    "    for _ in range(num_runs):\n",
    "        query = np.random.random((1,70))\n",
    "        \n",
    "        tic = time.time()\n",
    "        db_ids = db.retrieve(query, top_k)\n",
    "        toc = time.time()\n",
    "        run_time = toc - tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        actual_ids = np.argsort(np_rows.dot(query.T).T / (np.linalg.norm(np_rows, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]\n",
    "        toc = time.time()\n",
    "        np_run_time = toc - tic\n",
    "        \n",
    "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
    "    return results\n",
    "def run_queries_old(db, np_rows, top_k, num_runs):\n",
    "    results = []\n",
    "    for _ in range(num_runs):\n",
    "        query = np.random.random((1,70))\n",
    "        \n",
    "        tic = time.time()\n",
    "        db_ids = db.retrieve_old(query, top_k)\n",
    "        toc = time.time()\n",
    "        run_time = toc - tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        actual_ids = np.argsort(np_rows.dot(query.T).T / (np.linalg.norm(np_rows, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]\n",
    "        toc = time.time()\n",
    "        np_run_time = toc - tic\n",
    "        \n",
    "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
    "    return results\n",
    "\n",
    "def eval(results: List[Result]):\n",
    "    # scores are negative. So getting 0 is the best score.\n",
    "    scores = []\n",
    "    run_time = []\n",
    "    for res in results:\n",
    "        run_time.append(res.run_time)\n",
    "        # case for retrieving number not equal to top_k, score will be the lowest\n",
    "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
    "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
    "            continue\n",
    "        score = 0\n",
    "        for id in res.db_ids:\n",
    "            try:\n",
    "                ind = res.actual_ids.index(id)\n",
    "                if ind > res.top_k * 3:\n",
    "                    score -= ind\n",
    "            except:\n",
    "                score -= len(res.actual_ids)\n",
    "        scores.append(score)\n",
    "\n",
    "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db = VecDB(db_size= 10**4)\n",
    "\n",
    "    all_db = db.get_all_rows()\n",
    "\n",
    "    res1 = run_queries(db, all_db, 5, 10)\n",
    "    # res = run_queries_old(db, all_db, 5, 10)\n",
    "\n",
    "    print(eval(res1))\n",
    "    # print(eval(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
